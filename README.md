# Multi-Fusion-Net for Autonomous Driving

## ðŸŽ¯ Project Overview
Multi-modal (Camera + LiDAR) BEV-based perception model with adaptive camera configuration support.

### Key Features
- **Dynamic Multi-Camera**: Supports 2-6 cameras with automatic adaptation
- **LiDAR Fallback**: Works even when cameras are unavailable  
- **Multi-Task**: 3D Detection + Lane Segmentation + Occupancy mapping
- **Real-time**: Optimized for Jetson Orin (TensorRT INT8 â‰¥20 FPS)

### Dataset
- **PandaSet**: 39 sequences, 6-cam + 64ch LiDAR â†’ 32ch downsampled
- **Training**: Random camera dropout for robustness

## ðŸ—ï¸ Project Structure
```
multi_fusion_net/
â”œâ”€â”€ configs/           # Configuration files
â”œâ”€â”€ src/              # Source code
â”‚   â”œâ”€â”€ dataset/      # Data loaders
â”‚   â”œâ”€â”€ models/       # Model architectures
â”‚   â”œâ”€â”€ utils/        # Utility functions
â”‚   â””â”€â”€ losses/       # Loss functions
â”œâ”€â”€ scripts/          # Training/evaluation scripts
â”œâ”€â”€ data/            # Dataset symlink
â””â”€â”€ checkpoints/     # Model weights
```

## ðŸ”§ Development Phases
1. **Infrastructure**: Project setup, data loaders, coordinate transforms
2. **Model Architecture**: Camera backbone, LiDAR encoder, fusion module, heads
3. **Training System**: Loss functions, training scripts
4. **Deployment**: TensorRT optimization, real-time inference

## ðŸ“Š Target Performance
- **mAP**: >0.4 for 3D detection
- **Latency**: <50ms inference time
- **Memory**: <8GB GPU memory during training